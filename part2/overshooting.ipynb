{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141df72d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Estimate the mutual information between the images\n",
    "authors:\n",
    "  - name: Xu Wang\n",
    "  - name: Chung Chan\n",
    "math:\n",
    "  '\\abs': '\\left\\lvert #1 \\right\\rvert'\n",
    "  '\\norm': '\\left\\lvert #1 \\right\\rvert'\n",
    "  '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "  '\\mc': '\\mathcal{#1}'\n",
    "  '\\M': '\\boldsymbol{#1}'\n",
    "  '\\R': '\\mathsf{#1}'\n",
    "  '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "  '\\op': '\\operatorname{#1}'\n",
    "  '\\E': '\\op{E}'\n",
    "  '\\d': '\\mathrm{\\mathstrut d}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ed823",
   "metadata": {},
   "source": [
    "## Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50490efc",
   "metadata": {},
   "source": [
    ":::{note} Our model:\n",
    "\n",
    "\\begin{align}\n",
    "p_{XYC} &:= p_C p_{XY|C}\\\\\n",
    "&=p_C p_{X|C} p_{Y|C}, \n",
    "\\end{align}\n",
    "\n",
    "where $X,Y$ are hand-written digits in the MNIST dataset and $C$ is the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4ec78",
   "metadata": {},
   "source": [
    ":::{note} Assumptions\n",
    "\n",
    "In the model, we assume that\n",
    "1. $X,Y$ are i.i.d. distributed given $C$;\n",
    "2. $C$ is uniform distributed, i.e., $p_C = \\text{Unif}(\\{0,1,\\dots,9\\})$;\n",
    "3. $X$ can determine $C$, i.e., $H(C|X)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00f88e",
   "metadata": {},
   "source": [
    "Our goal is to estimate the mutual information between $X$ and $Y$ from limited samples of $p_{XY}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6705fa",
   "metadata": {},
   "source": [
    ":::{prf:proposition} \n",
    "\n",
    "Suppose the assumptions are satisfied, then we have\n",
    "$$\n",
    "I(X;Y) = \\log 10.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5b38b",
   "metadata": {},
   "source": [
    ":::{prf:proof}\n",
    "\\begin{align}\n",
    "I(X;Y) &\\leq \\underbrace{I(X;Y,C)}_{:= I(X;Y)+I(X;C|Y)} \\\\\n",
    "&= \\underbrace{I(X;X'|C)}_{=0 \\text{ by assumption 1}}+ \\underbrace{I(X;C)}_{:= H(C)- H(C|X) \\atop =H(C) \\text{ by assumption 3} }\\\\\n",
    "&= \\log 10,\n",
    "\\end{align}\n",
    "where the last step is because $C$ is uniform distributed by assumption 2. The equality holds because\n",
    "\\begin{align}\n",
    "I(X;C|Y) &\\leq H(C|Y)\\\\\n",
    "&=0\n",
    "\\end{align}\n",
    "by assumption 1 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f33e5c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from models.nn import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f3680",
   "metadata": {},
   "source": [
    "Choose a model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = 'DV_loss' # NWJ_loss ; DV_loss ; NCE_loss; choose the variational bounds to be optimized\n",
    "chkpt_name = f'./checkpoints/{loss_fn}_chkpt.pt'\n",
    "miest_type = 'all' # 'all': use the loss objective function as MI estimate; 'l1': use the sample average of net(XY) as the MI estimate\n",
    "reg = True # True: use the regularized loss function; False: use the original loss function\n",
    "refbatch = True # True: use a new reference batch; False: reshuffle the given batch data as the reference batch\n",
    "\n",
    "# initialize random seed\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST('../data/', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6187b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose how many digits to use\n",
    "test_labels = 10\n",
    "# each class contains 40 samples; thus we have 400 samples in total and the distribution of C is uniform\n",
    "class_size = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d074fc",
   "metadata": {},
   "source": [
    "Construct the dataset based on the assumptions: $X$ and $Y$ are i.i.d. distributed given $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282dd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data based on the labels\n",
    "grouped_data = {label: [] for label in range(10)}\n",
    "train_loaders = []\n",
    "\n",
    "for data in train_dataset:\n",
    "    label = data[1]\n",
    "    grouped_data[label].append(data)\n",
    "\n",
    "# construct the train loader for each class;\n",
    "for c in range(test_labels):\n",
    "    train_loaders.append(torch.utils.data.DataLoader(grouped_data[c], batch_size=class_size, shuffle=True))\n",
    "\n",
    "# construct the train dataset: X, Y are i.i.d. samples from the same class\n",
    "X = []\n",
    "Y = []\n",
    "for c in range(test_labels):\n",
    "    data1, _ = next(iter(train_loaders[c]))\n",
    "    data2, _ = next(iter(train_loaders[c]))\n",
    "    X.append(data1)\n",
    "    Y.append(data2)\n",
    "\n",
    "X = torch.cat(X)\n",
    "Y = torch.cat(Y)\n",
    "XY = torch.cat((X, Y), dim=1)\n",
    "\n",
    "# remove redundant data\n",
    "del grouped_data, train_loaders, train_dataset, data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068c697",
   "metadata": {},
   "source": [
    "## Initialize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e806b59",
   "metadata": {},
   "source": [
    "We will train a neural network with torch and use GPU if available. The structure of the neural network is shown below.\n",
    "\n",
    "<img src=\"./sources/nn.png\" alt=\"Neural Network Diagram\" width=\"600\" height=\"150\">\n",
    "\n",
    "where\n",
    "\n",
    "- $W_l$ and $b_l$ are the weight and bias respectively for the linear transformation $W_l z_{l-1} + b_l $ of the $l$-th layer;\n",
    "- $\\sigma$ is an activation function called [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f991f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network and optimizer\n",
    "# net is a fully connected neural network; the output is a scalar\n",
    "net = Linear_discriminator((1, 28, 28))\n",
    "# choose Adam as the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    X = X.cuda()\n",
    "    Y = Y.cuda()\n",
    "    XY = XY.cuda()\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313bde9",
   "metadata": {},
   "source": [
    "Load previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have saved the results of the previous run with 6000 iterations. The mi estimate has already overshooted the ground truth, \n",
    "# and you can continue to run the code to see where it will converge.\n",
    "\n",
    "load_available = True # set to False to prevent loading previous results\n",
    "if load_available and os.path.exists(chkpt_name):\n",
    "    checkpoint = torch.load(\n",
    "        chkpt_name, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mi_list = checkpoint['mi_list']\n",
    "    net.load_state_dict(checkpoint['net_state_list'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_list'])\n",
    "    print('Previous results loaded.')\n",
    "else:\n",
    "    mi_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42accef",
   "metadata": {},
   "source": [
    "Plot different digit pairs in the dataset ($X$,$Y$). $X_i$ and $Y_i$ have the same class label $C_i$ but with different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(10, 2, i*2+1)\n",
    "    plt.imshow(X[i*class_size][0].cpu().detach().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(10, 2, i*2+2)\n",
    "    plt.imshow(Y[i*class_size][0].cpu().detach().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d89cb5",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b0236",
   "metadata": {},
   "source": [
    ":::{tip} Question: **How to implements the mutual information estimate?**\n",
    "\n",
    "We will maximize DV lower bound as the example to estimate the mutual information between $X$ and $Y$. The empirical DV lower bound is\n",
    "\n",
    "$$\n",
    "\\sup_{f_{\\theta}:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow \\mathbb{R}} \\frac{1}{n}\\sum_{i\\in [n]} f_{\\theta}(X_i, Y_i) - \\log \\frac{1}{n'}\\sum_{i\\in [n']} e^{ f_{\\theta}(X'_i, Y'_i)},\n",
    "$$\n",
    "where\n",
    "- the supremum is over $f_{\\theta}$ represented by a neural network with trainable/optimizable parameters $\\theta$;\n",
    "- $P_{X'Y'} := P_XP_Y$;\n",
    "- $X'_i$ and $Y'_i$ are i.i.d. samples from $P_{X'Y'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610be40",
   "metadata": {},
   "source": [
    ":::{tip} Question: **How to get the samples from $P_{X'Y'}$?**\n",
    "\n",
    "Given $n$ samples $\\{X_i,Y_i\\}_{i\\in[n]}\\sim P_{XY}$, we can reshuflle the samples ${X_i}_{i\\in[n]}$, ${Y_i}_{i\\in[n]}$ and concatenate them together to get samples $\\{X'_i,Y'_i\\}_{i\\in[n^2]}\\sim P_{X'Y'}$. We can also draw a new reference batch of samples $\\{Y'_i\\}_{i\\in[m]}$ and apply the reshuflling trick to get $\\{X'_i,Y'_i\\}_{i\\in[mn]}\\sim P_{X'Y'}$. This is illustrated by the figure below.\n",
    "\n",
    "<img src=\"./sources/reshuffling.png\" alt=\"shuffle\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b1f38",
   "metadata": {},
   "source": [
    ":::{note} **Regularization**\n",
    "\n",
    "Since the original objective function has drifting issue, we can add a regularization term to avoid the output of the neural network drifting to infinity. The regularized loss function is\n",
    "\n",
    "$$\n",
    "L_{\\text{DV}}(\\theta) = -\\frac{1}{n}\\sum_{i\\in [n]} f_{\\theta}(X_i, Y_i) + \\log \\frac{1}{n'}\\sum_{i\\in [n']} e^{ f_{\\theta}(X'_i, Y'_i)} + \\lambda \\bigg(\\log \\frac{1}{n'}\\sum_{i\\in [n']} e^{ f_{\\theta}(X'_i, Y'_i)}\\bigg)^2,\n",
    "$$\n",
    "where $\\lambda>0$ is a hyperparameter to control the weight of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the training parameters\n",
    "max_iter = 8000 # maximum number of iterations\n",
    "batch_size = 40 # batch size in the mini-batch training\n",
    "ref_batch_size = batch_size # size for the reference batch if refbatch is True\n",
    "\n",
    "# if the loaded results are not enough, continue training\n",
    "continue_train = True\n",
    "if continue_train:\n",
    "    current_iter = len(mi_list)\n",
    "    print('Continue training from previous results, start from iter: ', current_iter)\n",
    "    for i in range(max_iter-len(mi_list)):\n",
    "        # draw mini-batch data; idx is the index of the mini-batch data\n",
    "        idx = resample(X, batch_size, return_idx=True)\n",
    "\n",
    "        # train\n",
    "        optimizer.zero_grad()\n",
    "        if refbatch: # use a different batch as the reference\n",
    "            # select a different batch as the reference\n",
    "            idx_ref = resample(Y, ref_batch_size, return_idx=True)\n",
    "            loss = eval(loss_fn)(net, X[idx], Y[idx], Y[idx_ref], reg=reg)\n",
    "        else: # Do not use new reference batch\n",
    "            loss = eval(loss_fn)(net, X[idx], Y[idx], reg=reg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate the MI\n",
    "        with torch.no_grad():\n",
    "            if miest_type == 'all': # estimate mi with the loss objective\n",
    "                mi_list.append(-eval(loss_fn)(net, X, Y).item())\n",
    "            elif miest_type == 'l1': # estimate mi with the average of net(XY) if net converges to the density ratio\n",
    "                mi_list.append(mi_estimate(net, XY).item())\n",
    "\n",
    "        # plot the MI estimation and the ground truth\n",
    "        if i % 400 == 0:\n",
    "            plt.clf()\n",
    "            EMA_SPAN = 50\n",
    "            p1 = plt.plot(mi_list, alpha=0.3, label='MI estimate')[0]\n",
    "            mis_smooth = pd.Series(mi_list).ewm(span=EMA_SPAN).mean()\n",
    "            plt.plot(mis_smooth, c=p1.get_color())\n",
    "            plt.axhline(math.log(XY.shape[0]), c='g', ls='--', label=f'log {XY.shape[0]}')\n",
    "            plt.axhline(math.log(test_labels), c='r', ls='--', label='ground truth')\n",
    "            plt.ylim(0, 1.2*math.log(XY.shape[0]))\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('MI estimation')\n",
    "            plt.title(f'iter: {i+current_iter:5d}, mi estimation: {mi_list[-1]:4f}')\n",
    "            plt.legend()\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "    display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fbd52",
   "metadata": {},
   "source": [
    "Save current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d13da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False  # set to True to overwrite previously stored results\n",
    "if overwrite or not os.path.exists(chkpt_name):\n",
    "    torch.save({\n",
    "        'mi_list': mi_list,\n",
    "        'net_state_list': net.state_dict(),\n",
    "        'optimizer_state_list': optimizer.state_dict(),\n",
    "    }, chkpt_name)\n",
    "    print('Current results saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
