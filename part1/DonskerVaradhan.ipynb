{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccdbdf8",
   "metadata": {},
   "source": [
    "---\n",
    "title: Neural Estimation via DV bound\n",
    "math:\n",
    "  '\\abs': '\\left\\lvert #1 \\right\\rvert'\n",
    "  '\\norm': '\\left\\lvert #1 \\right\\rvert'\n",
    "  '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "  '\\mc': '\\mathcal{#1}'\n",
    "  '\\M': '\\boldsymbol{#1}'\n",
    "  '\\R': '\\mathsf{#1}'\n",
    "  '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "  '\\op': '\\operatorname{#1}'\n",
    "  '\\E': '\\op{E}'\n",
    "  '\\d': '\\mathrm{\\mathstrut d}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6fc17",
   "metadata": {},
   "source": [
    "Since the $f$-divergence is nothing but an expectation, it may be tempting to approximate it by the sample average\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lim_{n\\to \\infty}\\frac1n \\sum_{i\\in [n]} f\\left(\\frac{d P_{\\R{Z}}}{d P_{\\R{Z}'}}(\\R{Z}'_i)\\right) \\xlongequal{\\text{a.s.}}\n",
    "E\\left[f\\left(\\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}(\\R{Z}')\\right)\\right] = D_f(P_{\\R{Z}} \\| P_{\\R{Z}'}), \n",
    "\\end{align}\n",
    "$$ (eq:avg-f-D)\n",
    "\n",
    "where the almost sure convergence to [](#eq:f-D) is by the law of large number in [](#thm:SLLN). \n",
    "\n",
    "::::{caution}\n",
    "\n",
    "Despite having the desired convergence, the sample average in [](#eq:avg-f-D) is invalid because calculating the sample average requires the knowledge of the unknown density ratio of $P_{\\R{Z}}$ to $P_{\\R{Z}'}$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5720f",
   "metadata": {},
   "source": [
    "This chapter aims to carefully unroll the idea of the neural estimation of divergence, namely, to train a neural network to tighten certain bound on the divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c28c3",
   "metadata": {},
   "source": [
    "## Neural estimation of KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423ae5c",
   "metadata": {},
   "source": [
    "The following result rewrites the KL divergence as an optimization over probability measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ba575",
   "metadata": {},
   "source": [
    "::::{prf:proposition}\n",
    ":label: pro:DV1\n",
    "\n",
    "The KL divergence in [](#eq:D) can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D(P_{\\R{Z}}\\|P_{\\R{Z}'}) & =  \\sup_{Q\\sim P_{\\R{Z}}} E \\left[ \\log \\frac{dQ}{dP_{\\R{Z}'}}(\\R{Z}) \\right],\n",
    "\\end{align}\n",
    "$$ (eq:D1)\n",
    "\n",
    "which has a unique solution $Q=P_{\\R{Z}}$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a940b30",
   "metadata": {},
   "source": [
    "::::{prf:proof}\n",
    ":class: dropdown\n",
    ":nonumber: true\n",
    "\n",
    "To prove [](#eq:D1),\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D(P_{\\R{Z}}\\|P_{\\R{Z}'})  &= D(P_{\\R{Z}}\\|P_{\\R{Z}'}) - \\inf_{Q\\sim P_{\\R{Z}}} \\underbrace{D(P_{\\R{Z}}\\|Q)}_{\\geq 0 \\text{ with equality iff } Q=P_{\\R{Z}}\\kern-3em} \\\\\n",
    "&= \\sup_{Q\\sim P_{\\R{Z}}}  \\underbrace{D(P_{\\R{Z}}\\|P_{\\R{Z}'})}_{=E \\left[\\log\\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}(\\R{Z})\\right]} -  \\underbrace{D(P_{\\R{Z}}\\|Q)}_{=E \\left[\\log\\frac{dP_{\\R{Z}}}{dQ}(\\R{Z})\\right]}\\\\\n",
    "&= \\sup_{Q\\sim P_{\\R{Z}}} E \\left[\\log\\frac{dQ}{dP_{\\R{Z}'}}(\\R{Z})\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the first inequality is by the positivity of KL divergence. The equality condition gives the unique solution as desired.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d250e82",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "[](#eq:D1) is the same as the definition in [](#eq:D) except that $P_{\\R{Z}}$ is replaced by an arbitrary parameter $Q$.\n",
    "\n",
    "- It essentially gives a tight lower bound on KL divergence.\n",
    "- The unknown distribution is recovered as the optimal solution.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74200a8",
   "metadata": {},
   "source": [
    "The idea of neural estimation is to \n",
    "\n",
    "1. estimate the expectation in [](#eq:D1) by the sample average\n",
    "  \n",
    "  $$\n",
    "  \\frac1n \\sum_{i\\in [n]} \\log \\underbrace{\\frac{dQ}{dP_{\\R{Z}'}}(\\R{Z})}_{\\text{(*)}},\n",
    "  $$\n",
    "  \n",
    "2. use a neural network to compute the density ratio (*), and\n",
    "3. train the network to maximize the expectation estimate, e.g., by gradient ascent on the above sample average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c18a2",
   "metadata": {},
   "source": [
    "## Donsker-Varadhan formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402305f1",
   "metadata": {},
   "source": [
    "**How to compute (*) without the knowledge of $P_{\\R{Z}'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3710c7",
   "metadata": {},
   "source": [
    "Consider a change of variable such as\n",
    "\n",
    "$$\n",
    "r(z) = \\frac{dQ(z)}{dP_{\\R{Z}'}(z)}\n",
    "$$ (eq:Q-r)\n",
    "\n",
    "to absorb the unknown reference into the parameter, which gives the following expression for KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f128b98",
   "metadata": {},
   "source": [
    "::::{prf:proposition}\n",
    ":label: pro:DV2\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D(P_{\\R{Z}}\\|P_{\\R{Z}'}) & =  \\sup_{\\substack{r\\in \\mathbb{R}^{\\Omega_{\\R{Z}}}_+\\\\ E[r(\\R{Z}')]=1}} E \\left[ \\log r(\\R{Z}) \\right] \n",
    "\\end{align}\n",
    "$$ (eq:D2)\n",
    "\n",
    "where the solution $r$ satisfies \n",
    "$\n",
    "r(\\R{Z}) \\xlongequal{\\text{a.s.}} \\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}(\\R{Z}).\n",
    "$ \n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21c2ad",
   "metadata": {},
   "source": [
    "::::{prf:proof}\n",
    ":class: dropdown\n",
    ":nonumber: true\n",
    "\n",
    "To prove [](#eq:D2), substitute [](#eq:Q-r) to [](#eq:D1). The constraint on $r$ is obtained from the constraint on $\\frac{dQ}{dP_{\\R{Z}}}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dQ}{dP_{\\R{Z}}}(z) \\geq 0 &\\iff r(z)\\geq 0\\\\\n",
    "\\int_{\\Omega_{\\R{Z}}}\\frac{dQ}{dP_{\\R{Z}}}dP_{\\R{Z}}=1 &\\iff E[r(\\R{Z}')]=1.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1594d",
   "metadata": {},
   "source": [
    "The next step is to have a neural network compute $r$ and train it to maximize the sample average estimate\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sup_{\\substack{r\\in \\mathbb{R}^{\\Omega_{\\R{Z}}}_+\\\\ \\frac1{n'}\\sum_{i\\in [n']} r(\\R{Z}'_i)]=1}} \\frac1n \\sum_{i\\in [n]} \\log r(\\R{Z}_i).\n",
    "\\end{align}\n",
    "$$ (eq:avg-D1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5fcfa",
   "metadata": {},
   "source": [
    "**How to impose the constraint on $r$ when training a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1742a",
   "metadata": {},
   "source": [
    "Consider another change of variable\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r(z)&=\\frac{e^{t(z)}}{E[e^{t(\\R{Z}')}]}.\n",
    "\\end{align}\n",
    "$$ (eq:r-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fb53a",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:r-t\n",
    "\n",
    "Show that $r$ defined in [](#eq:r-t) satisfies the constraint in [](#eq:D2) for all real-valued function $t\\in \\mathbb{R}^{\\Omega_{\\R{Z}}}$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8704164",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f67663e6b29a5a92a6692080e59aba7",
     "grade": true,
     "grade_id": "r-t",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea060572",
   "metadata": {},
   "source": [
    "Substituting [](#eq:r-t) into [](#eq:D2) gives the well-known *Donsker-Varadhan (DV)* formula [](https://doi.org/10.1002/cpa.3160360204):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f2255",
   "metadata": {},
   "source": [
    "::::{prf:theorem} DV formula\n",
    ":label: thm:DV\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D(P_{\\R{Z}}\\|P_{\\R{Z}'}) =  \\sup_{t\\in \\mathbb{R}^{\\Omega_{\\R{Z}}}} E[t(\\R{Z})] - \\log E[e^{t(\\R{Z}')}]\n",
    "\\end{align}\n",
    "$$ (eq:DV)\n",
    "\n",
    "where the optimal $t$ satisfies\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "t(\\R{Z}) \\xlongequal{\\text{a.s.}} \\log \\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}(\\R{Z}) + c\n",
    "\\end{align}\n",
    "$$ (eq:DV:sol)\n",
    "\n",
    "almost surely for some constant $c$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c6959",
   "metadata": {},
   "source": [
    "::::{prf:proof}\n",
    ":class: dropdown\n",
    ":nonumber: true\n",
    "\n",
    "To prove [](#eq:DV), substitute [](#eq:r-t) to [](#eq:D2) and argue that the constraint of $r$ in [](#eq:D2) is satisfied by [](#eq:r-t) as in [](#ex:r-t).\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab69e3",
   "metadata": {},
   "source": [
    "The divergence can be estimated as follows instead of [](#eq:avg-D1):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2838d4c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\sup_{t\\in \\mathbb{R}^{\\Omega_{\\R{Z}}}} \\frac1n \\sum_{i\\in [n]} t(\\R{Z}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t(\\R{Z}'_i)}\n",
    "\\end{align}\n",
    "$$ (eq:avg-DV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8bb062",
   "metadata": {},
   "source": [
    "In summary, the neural estimation of KL divergence is a sample average of [](#eq:D) but with the unknown density ratio replaced by [](#eq:r-t)\n",
    "\n",
    "$$\n",
    "D(P_{\\R{Z}}\\| P_{\\R{Z}'}) = \\underset{\\stackrel{\\uparrow}\\sup_t}{} \\overbrace{E}^{\\op{avg}} \\bigg[ \\log \\underbrace{\\frac{P_{\\R{Z}}}{P_{\\R{Z}'}}(\\R{Z})}_{\\frac{e^{t(\\R{Z})}}{\\underbrace{E}_{\\op{avg}}[e^{t(\\R{Z}')}]}} \\bigg].\n",
    "$$\n",
    "\n",
    "but with the unknown density ratio replaced by [](#eq:r-t) trained as a neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
