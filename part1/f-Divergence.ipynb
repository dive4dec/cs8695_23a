{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912aa515",
   "metadata": {},
   "source": [
    "---\n",
    "title: Extension of DV Formula\n",
    "math:\n",
    "  '\\abs': '\\left\\lvert #1 \\right\\rvert'\n",
    "  '\\norm': '\\left\\lvert #1 \\right\\rvert'\n",
    "  '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "  '\\mc': '\\mathcal{#1}'\n",
    "  '\\M': '\\boldsymbol{#1}'\n",
    "  '\\R': '\\mathsf{#1}'\n",
    "  '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "  '\\op': '\\operatorname{#1}'\n",
    "  '\\E': '\\op{E}'\n",
    "  '\\d': '\\mathrm{\\mathstrut d}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1bb2c",
   "metadata": {},
   "source": [
    "## $f$-divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9304d",
   "metadata": {},
   "source": [
    "Consider the more general problem of estimating the $f$-divergence in [](#eq:f-D):\n",
    "\n",
    "$$\n",
    "D_f(P_{\\R{Z}}\\|P_{\\R{Z}'}) = E\\left[f\\left(\\frac{dP_{\\R{Z}}(\\R{Z}')}{dP_{\\R{Z}'}(\\R{Z}')}\\right)\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24a594",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:DV-f-D\n",
    "\n",
    "How to estimate $f$-divergence using the DV formula?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d598d79",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93a318c2544d2cd8dab06d0ec46d3640",
     "grade": true,
     "grade_id": "DV-f-D",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ad5f9",
   "metadata": {},
   "source": [
    "Instead of using the DV bound, it is desirable to train a network to optimize a tight bound on the $f$-divergence because:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48806793",
   "metadata": {},
   "source": [
    "- *Estimating KL divergence well does not imply the underlying neural network approximates the density ratio well*:  \n",
    "  - While KL divergence is just a non-negative real number, \n",
    "  - the density ratio is in a high dimensional function space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed936efe",
   "metadata": {},
   "source": [
    "- DV formula does not directly maximizes a bound on $f$-divergence, i.e.  \n",
    "  it does not directly minimize the error in estimating $f$-divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf8388",
   "metadata": {},
   "source": [
    "- $f$-divergence may have bounds that are easier/more stable for training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f85ba",
   "metadata": {},
   "source": [
    "**How to extend the DV formula to $f$-divergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82984bd",
   "metadata": {},
   "source": [
    "The idea is to think of the $f$-divergence as a convex *function(al)* evaluated at the density ratio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9af1c",
   "metadata": {},
   "source": [
    "````{prf:proposition} \n",
    ":label: D-F\n",
    "\n",
    "$f$-divergence [](#eq:f-D) is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_f(P_{\\R{Z}}\\|P_{\\R{Z}'}) = F\\left[ \\frac{P_{\\R{Z}}}{P_{\\R{Z}'}}\\right]\n",
    "\\end{align}\n",
    "$$ (eq:D-F)\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F[r] := E [ \\underbrace{(f \\circ r)(\\R{Z}')}_{f(r(\\R{Z}'))}]\n",
    "\\end{align}\n",
    "$$ (eq:F)\n",
    "\n",
    "for any function $r:\\mc{Z} \\to \\mathbb{R}$.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c109834",
   "metadata": {},
   "source": [
    "This is more like a re-definition than a proposition as the proof is immediate:  \n",
    "[](#eq:f-D) is obtained from [](#eq:D-F) by substituting $r=\\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45297a67",
   "metadata": {},
   "source": [
    "As mentioned before, the KL divergence $D(P_{\\R{Z}}\\|P_{\\R{Z}'})$ is a special case of $f$-divergence:\n",
    "\n",
    "$$\n",
    "D(P_{\\R{Z}}\\|P_{\\R{Z}'}) = F\\left[r\\right]\n",
    "$$ \n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F[r] &:= E\\left[ r(\\R{Z}')\\log r(\\R{Z}')\\right].\n",
    "\\end{align*}\n",
    "$$ (eq:KL:F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31448115",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:zero-F\n",
    "When is $F[r]=0$ equal to $0$?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe6340",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "721d8acb3b106e887abb1900695639ac",
     "grade": true,
     "grade_id": "zero-F",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b5218",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:F-strictly-convex\n",
    "\n",
    "Show using the properties of $f$ that $F$ is strictly convex.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f483b8",
   "metadata": {},
   "source": [
    "::::{solution} ex:F-strictly-convex\n",
    ":class: dropdown\n",
    "\n",
    "For $\\lambda\\in [0,1]$ and functions $r_1, r_2\\in \\Set{r:\\mc{Z}\\to \\mathbb{R}}$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F[\\lambda r_1 + (1-\\lambda) r_2] \n",
    "&= E[\\underbrace{ f(\\lambda r_1(\\R{Z}') + (1-\\lambda) r_2(\\R{Z}'))}_{\\stackrel{\\text{(a)}}{\\geq} \\lambda f(r_1(\\R{Z}'))+(1-\\lambda) f(r_2(\\R{Z}'))}]\\\\\n",
    "&\\stackrel{\\text{(b)}}{\\geq} \\lambda E[f(r_1(\\R{Z}'))] + (1-\\lambda) E[f(r_2(\\R{Z}'))]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where (a) is by the convexity of $f$, and (b) is by the linearity of expectation. $F$ is strictly convex because (b) is satisfied with equality iff (a) is almost surely.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d3113",
   "metadata": {},
   "source": [
    "For a clearer understanding, consider a different choice of $F$ for the KL divergence:\n",
    "\n",
    "$$\n",
    "D(P_{\\R{Z}}\\|P_{\\R{Z}'}) = F'\\left[r\\right]\n",
    "$$ \n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F'[r] &:= E\\left[ \\log r(\\R{Z})\\right].\n",
    "\\end{align*}\n",
    "$$ (eq:rev-KL:F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc6a30",
   "metadata": {},
   "source": [
    "Note that $F'$ in [](#eq:rev-KL:F) defined above is concave in $r$. In other words, [](#eq:D2)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D(P_{\\R{Z}}\\|P_{\\R{Z}'}) & =  \\sup_{\\substack{r:\\mc{Z}\\to \\mathbb{R}_+\\\\ E[r(\\R{Z}')]=1}} E \\left[ \\log r(\\R{Z}) \\right] \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "is maximizing a concave function and therefore has a unique solution, namely, $r=\\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}$. Here comes the tricky question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28841ddf",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:rev-KL\n",
    "\n",
    "Is KL divergence concave or convex in the density ratio $\\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}$? Note that $F$ defined in [](#eq:KL:F) is convex in $r$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff8a1a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba9cab6aee2e4d32a74fe5fcabae39ff",
     "grade": true,
     "grade_id": "rev-KL",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd77cc0",
   "metadata": {},
   "source": [
    "## Convex conjugation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d634d3",
   "metadata": {},
   "source": [
    "Given $P_{\\R{Z}'}\\in \\mc{P}(\\mc{Z})$, consider \n",
    "- a function space $\\mc{R}$, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mc{R} &\\supseteq \\Set{r:\\mathcal{Z}\\to \\mathbb{R}_+\\mid E\\left[r(\\R{Z}')\\right] = 1},\n",
    "\\end{align}\n",
    "$$ (eq:R)\n",
    "\n",
    "- a dual space $\\mc{T}$, and \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mc{T} &\\subseteq \\Set{t:\\mc{Z} \\to \\mathbb{R}}\n",
    "\\end{align}\n",
    "$$ (eq:T)\n",
    "\n",
    "- the corresponding inner product $\\langle\\cdot,\\cdot \\rangle$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle t,r \\rangle &= \\int_{z\\in \\mc{Z}} t(z) r(z) dP_{\\R{Z}'}(z) = E\\left[ t(\\R{Z}') r(\\R{Z}') \\right].\n",
    "\\end{align}\n",
    "$$ (eq:inner-prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b8996",
   "metadata": {},
   "source": [
    "The following is a generalization of DV formula for estimating $f$-divergence [](https://doi.org/10.1109/TIT.2010.2068870)[](https://doi.org/10.48550/arXiv.1206.4664):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62e409",
   "metadata": {},
   "source": [
    "````{prf:proposition} \n",
    ":label: convex-conjugate\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{f}(P_{\\R{Z}} \\| P_{\\R{Z}'}) = \\sup _{t\\in \\mc{T}} E[g(\\R{Z})] - F^*[t],\n",
    "\\end{align} \n",
    "$$ (eq:convex-conjugate2)\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F^*[t] = \\sup_{r\\in \\mc{R}} E[t(\\R{Z}') r(\\R{Z}')] - F[r].\n",
    "\\end{align}\n",
    "$$ (eq:convex-conjugate1)\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7313955",
   "metadata": {},
   "source": [
    "````{prf:proof} \n",
    "\n",
    "Note that the supremums in [](#eq:convex-conjugate1) and [](#eq:convex-conjugate2) are [Fenchel-Legendre transforms][FL]. Denoting the transform as $[\\cdot]^*$,\n",
    "\n",
    "$$\\underbrace{[[F]^*]^*}_{=F}\\left[\\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}\\right]$$\n",
    "\n",
    "gives [](#eq:convex-conjugate2) by expanding the outer/later transform. The equality is by the property that Fenchel-Legendre transform is its own inverse for strictly convex functional $F$. This completes the proof by [](#eq:D-F).\n",
    "\n",
    "[FL]: https://en.wikipedia.org/wiki/Convex_conjugate\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ccd63",
   "metadata": {},
   "source": [
    "The proof is illustrated in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460effa0",
   "metadata": {},
   "source": [
    "![$f$-Divergence](images/f-D.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6409e",
   "metadata": {},
   "source": [
    "Let's breakdown the details:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70a54a",
   "metadata": {},
   "source": [
    "**Step 1**\n",
    "\n",
    "For the purpose of the illustration, visualize the convex functional $F$ simply as a curve in 2D.\n",
    "\n",
    "![$f$-Divergence 1](images/f-D-Copy1.dio.svg)  \n",
    "\n",
    "The $f$-divergence is then the $y$-coordinate of a point on the curve indicated above, with $r$ being the density ratio $\\frac{dP_{\\R{Z}}}{dP_{\\R{Z}'}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cabbce",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "To obtain a lower bound on $F$, consider any tangent of the curve with an arbitrary slope $t\\cdot dP_{\\R{Z}'}$\n",
    "\n",
    "![$f$-Divergence 2](images/f-D-Copy2.dio.svg)\n",
    "\n",
    "The lower bound is given by the $y$-coordinate of a point on the tangent with $r$ being the density ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2ef8a",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:tangent-lower-bound\n",
    "\n",
    "Why is the $y$-coordinate of the tangent a lower bound on the $f$-divergence?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445900ad",
   "metadata": {},
   "source": [
    "::::{solution} ex:tangent-lower-bound\n",
    ":class: dropdown\n",
    "\n",
    "By the convexity of $F$, the tangent must be below $F$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c7d71",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "\n",
    "To calculate the lower bound, denote the $y$-intercept as $-F^*[t]$:\n",
    "\n",
    "![$f$-Divergence 3](images/f-D-Copy3.dio.svg)  \n",
    "\n",
    "Thinking of a function as nothing but a vector, the displacement from the $y$-intercept to the lower bound is given by the inner product of the slope and the density ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ee149",
   "metadata": {},
   "source": [
    "**Step 4**\n",
    "\n",
    "To make the bound tight, maximize the bound over the choice of the slope or $t$:\n",
    "\n",
    "![$f$-Divergence 4](images/f-D-Copy4.dio.svg) \n",
    "\n",
    "This gives the bound in [](#eq:convex-conjugate2). It remains to show [](#eq:convex-conjugate1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36eed5d",
   "metadata": {},
   "source": [
    "**Step 5**\n",
    "\n",
    "To compute the $y$-intercept or $F^*[t]$, let $r^*$ be the value of $r$ where the tangent touches the convex curve:\n",
    "\n",
    "![$f$-Divergence 5](images/f-D.dio.svg) \n",
    "\n",
    "The displacement from the point at $r^*$ to the $y$-intercept can be computed as the inner product of the slope and $r^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06af7c",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:f-D_KL\n",
    "\n",
    "Show that for the functional $F$ [](#eq:KL:F) defined for KL divergence,\n",
    "\n",
    "$$F^*[t]=\\log E[e^{t(\\R{Z}')}]$$\n",
    "\n",
    "with $\\mc{R}=\\Set{r:\\mc{Z}\\to \\mathbb{R}_+}$ and so [](#eq:convex-conjugate2) gives the DV formula [](#eq:DV) as a special case.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442cebf1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "016908a4892874d6b5cbd32d8cfe25c5",
     "grade": true,
     "grade_id": "f-D_KL",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
